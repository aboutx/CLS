# CLS
Addressing Multi-label Learning with Partial Labels: from Sample Selection to Label Selection

Abstract
Multi-label Learning with Partial Labels (ML-PL) aims to learn from training data, where each sample is annotated with part of positive labels while leaving the rest of positive labels unannotated. Existing methods to deal with such problem mainly focus on extending multi-label losses to estimate unannotated labels, further inducing a missing-robust network. However, training with single network could lead to confirmation bias (\textit{i.e.} the model tends to confirm its mistakes). To tackle this issue, we propose a novel learning paradigm termed Co-\textit{Label Selection} (CLS), where two networks feed forward all data and cooperate in a co-training manner for critical label selection which is beneficial for learning robust network. Different from traditional co-training based methods that networks select confident samples for each other, we start from a new perspective that two networks are encouraged to remove false-negative labels while keep training samples reserved. Meanwhile, considering the extreme positive-negative label imbalance in ML-PL that leads the model to focus on negative labels, we enforce the model to concentrate on positive labels by abandoning non-informative negative labels to alleviate such issue. By shifting the cooperation strategy from ``\textit{Sample Selection}'' to ``\textit{Label Selection}'', CLS avoids directly dropping samples and reserves training data in most extent, thus enhancing the utilization of supervised signals and the generalization of the learning model. {\color{black} Empirical results performed on various multi-label datasets demonstrate that our CLS is much superior to other state-of-the-art methods.
